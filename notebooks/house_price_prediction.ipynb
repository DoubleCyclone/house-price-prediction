{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPFt3LKFhsMXSDyILGm3ZLj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DoubleCyclone/house-price-prediction/blob/main/notebooks/house_price_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will be working with the \"**House Prices - Advanced Regression Techniques**\" dataset today to perform\n",
        "*   Exploratory Data Analysis\n",
        "*   Data Preprocessing\n",
        "*   Feature Engineering\n",
        "*   Model Building\n",
        "*   Evaluation\n",
        "*   and Visualisation\n",
        "\n",
        "First things first, I am going to mount google drive so that I can upload the dataset there and easily access it from the notebook.\n",
        "\n",
        "The dataset (and the competition) is at [Kaggle Competition/Dataset Link](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/overview)"
      ],
      "metadata": {
        "id": "raXe0IoNFeNb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount the Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "YLtSFPpKF_h0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's import the other dependencies as well\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.preprocessing import OneHotEncoder, PowerTransformer\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "jICfVFhTjH5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 - Exploratory Data Analysis\n",
        "I will start by reading the train.csv which is the training dataset and displaying a small portion of it."
      ],
      "metadata": {
        "id": "DgOPF8xYQlKx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the training and test datasets, examine their shapes and contents\n",
        "data_train = pd.read_csv('/content/drive/MyDrive/Colab_Materials/House_Price_Estimation/train.csv')\n",
        "data_test = pd.read_csv('/content/drive/MyDrive/Colab_Materials/House_Price_Estimation/test.csv')\n",
        "# I will drop the ID columns as they are not used for training models\n",
        "data_train = data_train.drop('Id', axis=1)\n",
        "data_test = data_test.drop('Id', axis=1)\n",
        "\n",
        "print(f\"Shape of the train dataset = {data_train.shape}\")\n",
        "data_train.head()"
      ],
      "metadata": {
        "id": "nhLmpGMNQt9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Shape of the test dataset = {data_test.shape}\")\n",
        "data_test.head()"
      ],
      "metadata": {
        "id": "DiEKeB8QQuv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seems like there are almost an equal amount of data in both datasets. There is also one more column in the train dataset called **SalePrice** is the sale price which will be the labels the model will learn from in this case. How about getting an idea of its distribution in the dataset?"
      ],
      "metadata": {
        "id": "2PqEfOgbe3Vg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the pandas.describe method to automatically generate valuable information about the dataset (only for a numerical variable in this case)\n",
        "print(data_train['SalePrice'].describe())\n",
        "\n",
        "# Plot the distribution of the SalePrice column\n",
        "plt.figure(figsize=(5, 5))\n",
        "sns.distplot(data_train['SalePrice'], color='g', bins=100, hist_kws={'alpha': 0.4});\n",
        "\n",
        "# Move the SalePrice (label)\n",
        "data_y = data_train['SalePrice']\n",
        "\n",
        "# Drop label from the DataFrame\n",
        "data_train.drop('SalePrice', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "ZFK1J-VHi9qA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By looking at the output of the pandas.describe function and the graph, we can see that most of the Sale Prices reside at around 180000 and the standard deviation is quite low. Meaning that the data has low variability. How do I decide if the standard deviation is low or not? Firstly, I calculate the range of the data which is **max - min**. In this case **755000 - 34900 = 720100**. If the standard deviation is close to the range, I can say that the variability is high but in our case, standard deviation is approximately 10x lesser than the range which lets us conclude that the standard deviation, thus the variability in Sale Prices is low. <br><br>\n",
        "Now let's see what type of data is stored in the training dataset."
      ],
      "metadata": {
        "id": "7hY70rueoidx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the list of unique data types\n",
        "list(set(data_train.dtypes.tolist()))"
      ],
      "metadata": {
        "id": "d-1rtxn2xDZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's store the numerical data in a new DataFrame so that we can use it easily."
      ],
      "metadata": {
        "id": "Gs102umTyHfH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new DataFrame for numerical data only\n",
        "train_num = data_train.select_dtypes(include = ['float64', 'int64'])\n",
        "train_num.head()"
      ],
      "metadata": {
        "id": "YCHB0s7yyUgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "While at that, let's plot the distributions of all these numerical features at the same time."
      ],
      "metadata": {
        "id": "qXf_qVPPyn7T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_num.hist(figsize=(20, 20), bins=50, xlabelsize=8, ylabelsize=8)"
      ],
      "metadata": {
        "id": "CmC5qyKdyusB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I considered removing some of these columns if they could be unrelated to a house's sale price but I think all of these are useful so they will be kept. Let's take a look at the categorical features this time."
      ],
      "metadata": {
        "id": "EGVL-lMYW6Do"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new DataFrame for categorical data only\n",
        "train_cat = data_train.select_dtypes(include = ['O'])\n",
        "train_cat.head()"
      ],
      "metadata": {
        "id": "ErxRL549YBSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seems like most of our features are categorical. Let's plot their distributions as well."
      ],
      "metadata": {
        "id": "RIUCfV1fjsGJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a composed plot with matplotlib.subplots()\n",
        "fig, axs = plt.subplots(8, 6, figsize=(24, 32))\n",
        "\n",
        "# Flatten the axes so that iterating is easier\n",
        "axs = axs.flatten()\n",
        "\n",
        "# Iterate through each feature/column and plot their countplots\n",
        "for i, col in enumerate(train_cat.columns):\n",
        "  sns.countplot(x=train_cat[col], ax=axs[i])\n",
        "  axs[i].tick_params(axis='x', rotation=90)\n",
        "\n",
        "# Use matplotlib.tight_layout() to prevent overlapping\n",
        "fig.tight_layout()\n"
      ],
      "metadata": {
        "id": "cGMniBLqZUf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As it is not desirable to directly enumerate categorical values. For example, it does not make sense to give paved a value of 0 and gravel a value of 1 for the **Street** feature. We would rather use an approach called **One-Hot Encoding** which is basically to give every unique entry its own binary value like;\n",
        "* Paved = 1\n",
        "* Gravel = 0\n",
        "\n",
        "so that this means there is a paved street and not gravel around the house. It should be noted that only one of these values can be 1 (True) at a time but all can be 0 (False) if that entry is missing. <br><br>\n",
        "Also, numerical features' ranges vary by a large margin in the dataset. If we take a look at the features like **FullBath** which is a number of full bathrooms above grade and **TotalLotArea** which can get a value up to 200000~ as can be seen from the plots. So we should definitely normalize the numerical features so that we end up with values that are standardized. For example, limiting their range to 0-1 interval. <br>\n",
        "This\n",
        "* helps models converge faster\n",
        "* prevents the **NaN** trap which is when a value exceeds the floating point precision limit so that eventually every number in the model became **NaN**\n",
        "\n",
        "There are a few normalization methods in my mind so let's try to decide on which one would be the best fit for our dataset.\n",
        "- **Linear Scaling** : Best when\n",
        "  - there are few or no outliers, and the outliers aren't extreme.\n",
        "  - distribution of the data is approximately uniform.\n",
        "- **Z-Score Scaling** : Best when\n",
        "  - the distribution resembles a normal distribution or something close to it.\n",
        "- **Log Scaling** : Best when\n",
        "  - low values of x have very high values of y\n",
        "  - as the values of x increase, values of y quickly decrease\n",
        "  \n",
        "I might use something completely different who knows..."
      ],
      "metadata": {
        "id": "QISbDMfQkBtz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 - Data Preprocessing\n",
        "This has been a huge information dump but lets just examine what portion of each feature is missing now to see if we want to remove anything."
      ],
      "metadata": {
        "id": "RQ_eFvfA_feQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the DataFrame.isna().sum() with a condition to get all the counts of nan values for every column that has at least one, then turn it into a percentage\n",
        "train_num.isna().sum()[data_train.isna().sum() > 0] / len(data_train) * 100"
      ],
      "metadata": {
        "id": "MXGS641-lrfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Do the same with categorical features\n",
        "train_cat.isna().sum()[data_train.isna().sum() > 0] / len(data_train) * 100"
      ],
      "metadata": {
        "id": "kvyD826kpSDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Turns out most of the features that can take the value of **NaN** are categorical and it makes sense. A house does not necessarily have to have Pool for example so it can have a value of 0 for each **One-Hot Encoded** feature. Even the numerical data makes sense as if a property does not have a garage for example, it should not have a **GarageYrBlt** value. Though, this is a problem as we do not want any **NaN** values in our dataset and I am going to only scale the numerical ones so the **NaN** values are not going to disappear. To remedy this, I will\n",
        "- Take the median for LotFrontage\n",
        "- Set MasVnrArea to 0\n",
        "- Set GarageYrBlt to the same as YearBuilt\n",
        "\n",
        "hopefully to get rid of those in the most reasonable way possible.<br><br>\n",
        "\n",
        "As it is the easiest part, let's turn categorical features into One-Hot Encodings.\n"
      ],
      "metadata": {
        "id": "LYAikrsZo1WQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle missing data\n",
        "train_num['LotFrontage'] = train_num['LotFrontage'].fillna(train_num['LotFrontage'].median())\n",
        "train_num['MasVnrArea'] = train_num['MasVnrArea'].fillna(0)\n",
        "train_num['GarageYrBlt'] = train_num['GarageYrBlt'].fillna(train_num['YearBuilt'])"
      ],
      "metadata": {
        "id": "HzaZNQAdS3cO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's take **One-Hot-Encode** the categorical features."
      ],
      "metadata": {
        "id": "3oAqzG7lTGZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the categorical columns\n",
        "categorical_columns = train_cat.columns.tolist()\n",
        "\n",
        "# Specify encoder\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "\n",
        "# Create a Tensor for the one-hot-encoded features\n",
        "one_hot_encoded = encoder.fit_transform(data_train[categorical_columns])\n",
        "\n",
        "# Create a DataFrame from the Tensor with the appropriate column names\n",
        "one_hot_df = pd.DataFrame(one_hot_encoded, columns=encoder.get_feature_names_out(categorical_columns))\n",
        "\n",
        "# Display a portion of it\n",
        "one_hot_df.head()"
      ],
      "metadata": {
        "id": "V58IC6hHAGuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have 267 columns of **True** or **False** values for categorical features. For the next step, let's try to use Scikit-Learn's PowerTransformer to normalize the data instead of the other three methods as I don't think any of them fit well to all (or even most) features."
      ],
      "metadata": {
        "id": "nfBZnDSQDAnJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the PowerTransformer\n",
        "pt = PowerTransformer(method='yeo-johnson', standardize=True)\n",
        "\n",
        "# Transform only the numerical the data\n",
        "train_num_normalized = pd.DataFrame(pt.fit_transform(train_num), columns=pt.get_feature_names_out(train_num.columns))\n",
        "\n",
        "# Plot them after the transformation\n",
        "train_num_normalized.hist(figsize=(20, 20), bins=50, xlabelsize=8, ylabelsize=8)"
      ],
      "metadata": {
        "id": "GU6MzOGAb4mo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These distributions look a lot better than how they were originally. The scale problem is solved as all the numerical features now span a similar range. Finally let's create the actual training dataset that we are going to use to train models."
      ],
      "metadata": {
        "id": "P-UpDUvXgf32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate the scaled numerical features and one-hot-encoded categorical values\n",
        "data_train_normalized = pd.concat([train_num_normalized, one_hot_df], axis=1)\n",
        "\n",
        "# Display a portion of it\n",
        "data_train_normalized.head()"
      ],
      "metadata": {
        "id": "NzSdDjcvgy_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that I have the **normalized** and **one-hot-encoded** features. I will convert this DataFrame into a **PyTorch Tensor** so that we can start use it for training easily. Let's convert the labels as well."
      ],
      "metadata": {
        "id": "P08mJHAI9l0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert DataFrame to a PyTorch Tensor\n",
        "X_train = torch.tensor(data_train_normalized.values, dtype=torch.float32)\n",
        "\n",
        "# Display the shape of the tensors\n",
        "print(f\"Training Data Shape : {X_train.shape}\")"
      ],
      "metadata": {
        "id": "rgDRrKQe-LmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that our data is ready to go. I will initialize the Linear Regression Model with **PyTorch**. I will be using **RMSE (Root Mean Squared Error)** as the loss function to abide by the competition rules. Turns out the loss is calculated between the logarithms of the Sale Prices so I will need to transform the label DataFrame. I will also use the **AdamW** optimizer."
      ],
      "metadata": {
        "id": "DGu5C2nKCXMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearRegressionModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim):\n",
        "        super(LinearRegressionModel, self).__init__()\n",
        "        self.linear = torch.nn.Linear(input_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y_pred = self.linear(x)\n",
        "        return y_pred"
      ],
      "metadata": {
        "id": "Mov_ghK6CgNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create RMSE Loss (MSE but inside a square root)\n",
        "class RMSELoss(torch.nn.Module):\n",
        "    def __init__(self, eps = 1e-6):\n",
        "        super(RMSELoss,self).__init__()\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self,x,y):\n",
        "        criterion = nn.MSELoss()\n",
        "        loss = torch.sqrt(criterion(x, y) + self.eps)\n",
        "        return loss"
      ],
      "metadata": {
        "id": "qfplLjWGKpYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model initialization\n",
        "lr_model = LinearRegressionModel(input_dim=X_train.shape[1])\n",
        "\n",
        "# Select Loss Function and optimizer\n",
        "criterion = RMSELoss(eps=1e-6)\n",
        "optimizer = torch.optim.AdamW(lr_model.parameters(), lr=0.01, weight_decay=0.01)"
      ],
      "metadata": {
        "id": "5yGc7SK7C8SX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform the labels by calculating their logarithms\n",
        "data_y_log = np.log1p(data_y)\n",
        "\n",
        "# Transform the labels to a tensor\n",
        "y_train = torch.tensor(data_y_log.values, dtype=torch.float32).view(-1, 1)\n",
        "print(f\"Training Labels Shape : {y_train.shape}\")"
      ],
      "metadata": {
        "id": "rjsbTMYsHNEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Okay. Everything should be ready to start our first model training. As I have few data (Yes 1460 entries is few...), I think training for 150 epochs should be reasonable. Don't forget that training for too long has the risk of overfitting the model."
      ],
      "metadata": {
        "id": "VtspsALTMgv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To track losses\n",
        "losses = []\n",
        "\n",
        "for epoch in range(150):\n",
        "\n",
        "    # Perform a forward pass (prediction)\n",
        "    pred_y = lr_model(X_train)\n",
        "\n",
        "    # Compute and print loss\n",
        "    loss = criterion(pred_y, y_train)\n",
        "\n",
        "    # Track losses\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    # Zero gradients, perform a backward pass, and update the weights\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"epoch {epoch + 1}, loss {loss.item()}\")\n",
        "\n",
        "# Plot loss function\n",
        "plt.plot(losses)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('RMSE Loss')\n",
        "plt.title('Training Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "i0cSDKROM2Vt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our loss function looks pretty good, it converges well. There is an oscillation between epochs ~24-75 but it stabilizes after that."
      ],
      "metadata": {
        "id": "42Igh8TPWCn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We aren't going to update the model so we use no_grad()\n",
        "with torch.no_grad():\n",
        "    predictions = lr_model(X_train).numpy()\n",
        "\n",
        "plt.scatter(y_train.numpy(), predictions, alpha=0.5)\n",
        "plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--')  # perfect line\n",
        "plt.xlabel('Actual (log price)')\n",
        "plt.ylabel('Predicted (log price)')\n",
        "plt.title('Predictions vs Actuals')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "thTPsC5cVHlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Points hug the line well enough. The correlation looks strong except for the slight spread at lower prices."
      ],
      "metadata": {
        "id": "RKFkqTzcWrkl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "residuals = y_train.numpy() - predictions\n",
        "\n",
        "plt.scatter(predictions, residuals, alpha=0.5)\n",
        "plt.axhline(y=0, color='r', linestyle='--')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Residual')\n",
        "plt.title('Residual Plot')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "30mzn0nCVRy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The points are randomly scattered around 0 and there is no obvious pattern which is a good indicator that the model is learning the relationships well."
      ],
      "metadata": {
        "id": "HuklutZGXdvs"
      }
    }
  ]
}